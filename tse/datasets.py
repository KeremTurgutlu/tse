# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03-datasets.ipynb (unless otherwise specified).

__all__ = ['SQUAD_DATA_PATH', 'PRETRAINED_TOK_PATH', 'TSEDataAugmentor', 'SQUAD_Dataset', 'get_fold_ds',
           'predict_answer_text']

# Cell
from fastai.text import *
from transformers import AutoTokenizer

from .squad_utils import *
from .tokenizers import *

# Cell
SQUAD_DATA_PATH = Path("../squad_data/")
PRETRAINED_TOK_PATH = Path("../tokenizers/roberta-base/")

# Cell
class TSEDataAugmentor():

    def __init__(self, tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map):

        self.tokenizer = tokenizer
        self.input_ids = input_ids
        self.attention_mask = attention_mask

        # initial answer start and end positions
        self.ans_start_pos, self.ans_end_pos = start_position.item(), end_position.item()

        # initial context start and end positions
        self.token_to_orig_map = token_to_orig_map
        self.context_start_pos, self.context_end_pos = min(token_to_orig_map), max(token_to_orig_map)



    # left and right indexes excluding answer tokens and eos token
    @property
    def left_idxs(self): return np.arange(self.context_start_pos, self.ans_start_pos)

    @property
    def right_idxs(self): return np.arange(self.ans_end_pos+1, self.context_end_pos+1)

    @property
    def left_right_idxs(self): return np.concatenate([self.left_idxs, self.right_idxs])

    @property
    def rand_left_idx(self): return np.random.choice(self.left_idxs) if self.left_idxs.size > 0 else None

    @property
    def rand_right_idx(self): return np.random.choice(self.right_idxs) if self.right_idxs.size > 0 else None



    def right_truncate(self, right_idx):
        """
        Truncate context from random right index to beginning, answer pos doesn't change
        Note: token_type_ids NotImplemented
        """
        if not right_idx: raise Exception("Right index can't be None")

        # clone for debugging
        new_input_ids = self.input_ids.clone()
        nopad_input_ids = new_input_ids[self.attention_mask.bool()]

        # truncate from right idx to beginning - add eos_token_id to end
        truncated = torch.cat([nopad_input_ids[:right_idx+1], tensor([self.tokenizer.eos_token_id])])

        # pad new context until size are equal
        # replace original input context with new
        n_pad = len(nopad_input_ids) - len(truncated)
        new_context = F.pad(truncated, (0,n_pad), value=self.tokenizer.pad_token_id)
        new_input_ids[:self.context_end_pos+2] = new_context


        # find new attention mask, update new context end position (exclude eos token)
        # Note: context start doesn't change since we don't manipulate question
        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])
        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1
        self.context_end_pos = new_context_end_pos

        # update input_ids and attention_masks
        self.input_ids = new_input_ids
        self.attention_mask = new_attention_mask

        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))

    def random_right_truncate(self):
        right_idx = self.rand_right_idx
        if right_idx: self.right_truncate(right_idx)


    def left_truncate(self, left_idx):
        """
        Truncate context from random left index to end, answer pos changes too
        Note: token_type_ids NotImplemented
        """

        if not left_idx: raise Exception("Left index can't be None")

        # clone for debugging
        new_input_ids = self.input_ids.clone()

        # pad new context until size are equal
        # replace original input context with new

        n_pad = len(new_input_ids[self.context_start_pos:]) - len(new_input_ids[left_idx:])

        new_context = F.pad(new_input_ids[left_idx:], (0,n_pad), value=self.tokenizer.pad_token_id)

        new_input_ids[self.context_start_pos:] = new_context


        # find new attention mask, update new context end position (exclude eos token)
        # Note: context start doesn't change since we don't manipulate question
        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])
        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1
        self.context_end_pos = new_context_end_pos

        # find new answer start and end positions
        # update new answer start and end positions
        ans_shift = left_idx - self.context_start_pos
        self.ans_start_pos, self.ans_end_pos = self.ans_start_pos-ans_shift, self.ans_end_pos-ans_shift


        # update input_ids and attention_masks
        self.input_ids = new_input_ids
        self.attention_mask = new_attention_mask

        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))

    def random_left_truncate(self):
        left_idx = self.rand_left_idx
        if left_idx: self.left_truncate(left_idx)


    def replace_with_mask(self, idxs_to_mask):
        """
        Replace given input ids with tokenizer.mask_token_id
        """
        # clone for debugging
        new_input_ids = self.input_ids.clone()
        new_input_ids[idxs_to_mask] = tensor([tokenizer.mask_token_id]*len(idxs_to_mask))
        self.input_ids = new_input_ids


    def random_replace_with_mask(self, mask_p=0.2):
        """
        mask_p: Proportion of tokens to replace with mask token id
        """
        idxs_to_mask = np.random.choice(self.left_right_idxs, int(len(self.left_right_idxs)*mask_p))
        if idxs_to_mask.size > 0: self.replace_with_mask(idxs_to_mask)



# Cell
class SQUAD_Dataset(Dataset):
    def __init__(self, tokenizer, dataset_tensors, examples, features, is_training=True, do_tfms=None):
        self.dataset_tensors = dataset_tensors
        self.examples = examples
        self.features = features
        self.is_training = is_training
        self.tokenizer = tokenizer
        self.do_tfms = do_tfms


    def __getitem__(self, idx):
        'fastai requires (xb, yb) to return'

        input_ids = self.dataset_tensors[0][idx]
        attention_mask = self.dataset_tensors[1][idx]
        token_type_ids = self.dataset_tensors[2][idx]
        if self.is_training:
            start_position = self.dataset_tensors[3][idx]
            end_position = self.dataset_tensors[4][idx]

            if self.do_tfms:
                token_to_orig_map = self.features[idx].token_to_orig_map

                augmentor = TSEDataAugmentor(self.tokenizer,
                                             input_ids,
                                             attention_mask,
                                             start_position, end_position,
                                             token_to_orig_map)

                if np.random.uniform() < self.do_tfms["random_left_truncate"]["p"]:
                    augmentor.random_left_truncate()
                if np.random.uniform() < self.do_tfms["random_right_truncate"]["p"]:
                    augmentor.random_right_truncate()
                if np.random.uniform() < self.do_tfms["random_replace_with_mask"]["p"]:
                    augmentor.random_replace_with_mask(self.do_tfms["random_replace_with_mask"]["mask_p"])

                input_ids = augmentor.input_ids
                attention_mask = augmentor.attention_mask
                start_position, end_position = tensor(augmentor.ans_start_pos), tensor(augmentor.ans_end_pos)


        xb = (input_ids, attention_mask, token_type_ids)
        if self.is_training: yb = (start_position, end_position)
        else: yb = 0

        return xb, yb

    def __len__(self): return len(self.dataset_tensors[0])

# Cell
def get_fold_ds(foldnum, tokenizer):
    data_dir = "/kaggle/working/squad_data"
    train_filename = f"train_squad_data_{foldnum}.json"
    valid_filename = f"valid_squad_data_{foldnum}.json"
    test_filename = "test_squad_data.json"

    # examples
    train_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'train_squad_data_0.json')
    valid_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'valid_squad_data_0.json')
    test_examples = squad_processor.get_dev_examples(SQUAD_DATA_PATH, 'test_squad_data.json')

    # features and tensors
    train_features, train_dataset = get_squad_dataset(train_examples, tokenizer, True)
    valid_features, valid_dataset = get_squad_dataset(valid_examples, tokenizer, True)
    test_features, test_dataset = get_squad_dataset(test_examples, tokenizer, False)
    train_dataset_tensors = train_dataset.tensors
    valid_dataset_tensors = valid_dataset.tensors
    test_dataset_tensors = test_dataset.tensors

    # create pytorch dataset
    do_tfms = {}
    do_tfms["random_left_truncate"] = {"p":0.3}
    do_tfms["random_right_truncate"] = {"p":0.3}
    do_tfms["random_replace_with_mask"] = {"p":0.3, "mask_p":0.3}

    train_ds = SQUAD_Dataset(tokenizer, train_dataset_tensors, train_examples, train_features, True, do_tfms)
    valid_ds = SQUAD_Dataset(tokenizer, valid_dataset_tensors, valid_examples, valid_features, True)
    test_ds = SQUAD_Dataset(tokenizer, test_dataset_tensors, test_examples, test_features, False)

    return train_ds, valid_ds, test_ds

# Cell
def predict_answer_text(start_logits, end_logits, attention_mask,
                        context_text, char_to_word_offset, token_to_orig_map):
    "Find best answer from context"
    # find best start and end
    context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)
    truncated_start_logits = start_logits[attention_mask.bool()][context_start:context_end+1]
    truncated_end_logits = end_logits[attention_mask.bool()][context_start:context_end+1]
    best_start_idx, best_end_idx = find_best_start_end_idxs(truncated_start_logits, truncated_end_logits)

    # generate answer
    tok_orig_char_start = token_to_orig_map[best_start_idx+context_start]
    tok_orig_char_end = token_to_orig_map[best_end_idx+context_start]
    return answer_from_orig_context(context_text, char_to_word_offset, tok_orig_char_start, tok_orig_char_end)