{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Tokenizers\n",
    "\n",
    ">Exploring tokenizers offered and used by transformers models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.core import *\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaTokenizer\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_TYPE = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TYPE, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_TOKENIZERS_PATH = Path(\"../tokenizers\")\n",
    "os.makedirs(PRETRAINED_TOKENIZERS_PATH, exist_ok=True)\n",
    "os.makedirs(PRETRAINED_TOKENIZERS_PATH/PRETRAINED_TYPE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PosixPath('../tokenizers/roberta-base'),\n",
       "  PosixPath('../tokenizers/tf-roberta')],\n",
       " [PosixPath('../tokenizers/roberta-base/tokenizer_config.json'),\n",
       "  PosixPath('../tokenizers/roberta-base/special_tokens_map.json'),\n",
       "  PosixPath('../tokenizers/roberta-base/merges.txt'),\n",
       "  PosixPath('../tokenizers/roberta-base/vocab.json')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(PRETRAINED_TOKENIZERS_PATH).ls(), (PRETRAINED_TOKENIZERS_PATH/PRETRAINED_TYPE).ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../tokenizers/roberta-base/vocab.json',\n",
       " '../tokenizers/roberta-base/merges.txt',\n",
       " '../tokenizers/roberta-base/special_tokens_map.json',\n",
       " '../tokenizers/roberta-base/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(PRETRAINED_TOKENIZERS_PATH/PRETRAINED_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Roberta uses GPT-2 tokenizer model - ByteLevelBPETokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Which tokenizer model Roberta uses?\"\n",
    "q2 = \"Which model uses GPT-2 tokenizer?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Roberta uses GPT-2 tokenizer model - ByteLevelBPETokenizer</s>'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(s), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need do_lower_case=False for roberta-base\n",
    "tokenizer = RobertaTokenizer.from_pretrained(str(PRETRAINED_TOKENIZERS_PATH/PRETRAINED_TYPE), \n",
    "                                             do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Roberta uses GPT-2 tokenizer model - ByteLevelBPETokenizer</s>'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(s), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "\n",
    "`tokenizers` package have the following tokenizers:\n",
    "\n",
    "- ByteLevelBPETokenizer\n",
    "- CharBPETokenizer\n",
    "- SentencePieceBPETokenizer\n",
    "- BertWordPieceTokenizer\n",
    "\n",
    "There are many models offered and each of these pretrained models use a specific tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tse.squad_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_TOK_PATH = PRETRAINED_TOKENIZERS_PATH/PRETRAINED_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../tokenizers/roberta-base/tokenizer_config.json'),\n",
       " PosixPath('../tokenizers/roberta-base/special_tokens_map.json'),\n",
       " PosixPath('../tokenizers/roberta-base/merges.txt'),\n",
       " PosixPath('../tokenizers/roberta-base/vocab.json')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_TOK_PATH.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer, CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer, BertWordPieceTokenizer)\n",
    "from tokenizers.processors import RobertaProcessing, BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_conf_args = read_json_as_dict(PRETRAINED_TOK_PATH/'tokenizer_config.json')\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=str(PRETRAINED_TOK_PATH/'vocab.json'), \n",
    "    merges_file=str(PRETRAINED_TOK_PATH/'merges.txt'), \n",
    "    lowercase=tok_conf_args['do_lower_case'],\n",
    "    add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/tokenizers_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From encoded string we can access the following attributes:\n",
    "\n",
    "- `ids`: token ids\n",
    "- `attention_mask`: binary indicator of what's padded or not\n",
    "- `type_ids`: token type ids for some models, such as BERT\n",
    "- `offsets`: offsets to map tokens back to original string positions\n",
    "- `original_str`: original string\n",
    "- `normalized_str`: original string after normalizer step\n",
    "\n",
    "Also the following methods:\n",
    "\n",
    "-  `truncate()`: to truncate text to a length\n",
    "-  `pad()`: to pad text to a length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=18, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])\n"
     ]
    }
   ],
   "source": [
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4533,  6747,   102,  2939,   821,  3320,    12,   176, 19233,  6315,  1421,   111, 47893,  4483,   428, 13713,\n",
       "        22036,  6315]),\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(toks.ids), toks.attention_mask, array(toks.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer won't have special token handling yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = read_json_as_dict(PRETRAINED_TOK_PATH/'special_tokens_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0\n",
      "</s> 2\n",
      "<unk> 3\n",
      "</s> 2\n",
      "<pad> 1\n",
      "<s> 0\n",
      "<mask> 50264\n"
     ]
    }
   ],
   "source": [
    "for k,v in special_tokens.items(): print(v, tokenizer.token_to_id(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab seems to already have these special tokens since we are using a pretrained tokenizers `vocab.json`. Let's add these as attributes to our `tokenizer` instance for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in special_tokens.items(): setattr(tokenizer, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenizer.bos_token, tokenizer.eos_token, tokenizer.unk_token, tokenizer.sep_token, tokenizer.pad_token, \n",
    "tokenizer.cls_token, tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method 'num_special_tokens_to_add' of 'PostProcessor' objects>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertProcessing.num_special_tokens_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Tokenizers\n",
    "\n",
    "We can alternatively use tokenizer and processors implemented in `transformers` without much hassle. We will use `AutoTokenizer` to load the same pretrained tokenizer. This will handle special tokens for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../tokenizers/roberta-base/tokenizer_config.json'),\n",
       " PosixPath('../tokenizers/roberta-base/special_tokens_map.json'),\n",
       " PosixPath('../tokenizers/roberta-base/config.json'),\n",
       " PosixPath('../tokenizers/roberta-base/merges.txt'),\n",
       " PosixPath('../tokenizers/roberta-base/vocab.json')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_TOK_PATH.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {PRETRAINED_TOK_PATH/'tokenizer_config.json'} {PRETRAINED_TOK_PATH/'config.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(str(PRETRAINED_TOK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> roberta uses gpt-2 tokenizer model - bytelevelbpetokenizer</s></s> which tokenizer model roberta uses?</s>'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(s, q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use SQUAD processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "squad_processor = SquadV2Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DATA_PATH = Path(\"../squad_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21984/21984 [00:00<00:00, 23792.46it/s]\n",
      "100%|██████████| 5496/5496 [00:00<00:00, 29336.36it/s]\n",
      "100%|██████████| 3534/3534 [00:00<00:00, 29409.30it/s]\n"
     ]
    }
   ],
   "source": [
    "train_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'train_squad_data_0.json')\n",
    "valid_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'valid_squad_data_0.json')\n",
    "test_examples = squad_processor.get_dev_examples(SQUAD_DATA_PATH, 'test_squad_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = train_examples[0].question_text\n",
    "answer = train_examples[0].answer_text\n",
    "context = train_examples[0].context_text \n",
    "char_to_offset = array(train_examples[0].char_to_word_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " 'Sooo SAD',\n",
       " ' Sooo SAD I will miss you here in San Diego!!!',\n",
       " array([-1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,\n",
       "         6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, answer, context, char_to_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MAX_SEQ_LEN = 104\n",
    "MAX_QUERY_LEN = 5\n",
    "DOC_STRIDE = 200 # useful for LM modeling\n",
    "def get_squad_dataset(examples, tokenizer, is_training):\n",
    "    return squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        doc_stride=DOC_STRIDE,\n",
    "        max_seq_length=MAX_SEQ_LEN,\n",
    "        max_query_length=MAX_QUERY_LEN,\n",
    "        is_training=is_training,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=defaults.cpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 21984/21984 [00:06<00:00, 3339.75it/s]\n",
      "add example index and unique id: 100%|██████████| 21984/21984 [00:00<00:00, 838891.33it/s]\n",
      "convert squad examples to features: 100%|██████████| 5496/5496 [00:01<00:00, 3617.62it/s]\n",
      "add example index and unique id: 100%|██████████| 5496/5496 [00:00<00:00, 830968.41it/s]\n",
      "convert squad examples to features: 100%|██████████| 3534/3534 [00:00<00:00, 4321.36it/s]\n",
      "add example index and unique id: 100%|██████████| 3534/3534 [00:00<00:00, 723374.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_dataset = get_squad_dataset(train_examples, tokenizer, True)\n",
    "valid_features, valid_dataset = get_squad_dataset(valid_examples, tokenizer, True)\n",
    "test_features, test_dataset = get_squad_dataset(test_examples, tokenizer, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that we can extract `input_ids`, `attention_mask`, `token_type_ids` (not used by Roberta) for modeling. `token_to_orig_map` can be used for mapping tokens back to original string. Note that this dictionary starts with `4th idx` token since first 4 are our question `['<s>', 'negative', '</s>', '</s>']` tokens, also the last one is not used since it is eos token `'</s>'`. We need take this into account when mapping back from tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " {4: 0,\n",
       "  5: 0,\n",
       "  6: 1,\n",
       "  7: 1,\n",
       "  8: 2,\n",
       "  9: 3,\n",
       "  10: 4,\n",
       "  11: 5,\n",
       "  12: 6,\n",
       "  13: 7,\n",
       "  14: 8,\n",
       "  15: 9,\n",
       "  16: 9,\n",
       "  17: 9})"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_orig_map = train_features[0].token_to_orig_map\n",
    "context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "len(token_to_orig_map), token_to_orig_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,\n",
       " array(['<s>', 'negative', '</s>', '</s>', 'so', 'oo', 's', 'ad', 'i', 'will', 'miss', 'you', 'here', 'in', 'san',\n",
       "        'die', 'go', '!!!', '</s>'], dtype='<U8'))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = array(train_features[0].tokens)\n",
    "len(tokens), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " array(['so', 'oo', 's', 'ad', 'i', 'will', 'miss', 'you', 'here', 'in', 'san', 'die', 'go', '!!!'], dtype='<U8'))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[context_start:context_end+1]), tokens[context_start:context_end+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end predictions may contain indexes from `(0, MAX_SEQ_LEN)` so we will need some post processing to map back to original string for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = train_dataset.tensors\n",
    "\n",
    "input_ids = train_tensors[0][0]\n",
    "attention_mask = train_tensors[1][0]\n",
    "token_type_ids = train_tensors[2][0]\n",
    "start_position = train_tensors[3][0].item()\n",
    "end_position = train_tensors[4][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([104]), torch.Size([104]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "start_logits = torch.randn_like(input_ids, dtype=torch.float)\n",
    "end_logits = torch.randn_like(input_ids, dtype=torch.float)\n",
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter `start_logits` and `end_logits` before finding the best start and end idxs:\n",
    "\n",
    "- Filter by `attention_mask` to exclude padding\n",
    "- Filter by question tokens [4:] first 4 tokens in our case, which can be also obtained from `min() and max()` keys of `token_to_orig_map` for variable question lengths\n",
    "- Exclude final token [:-1] which is the `eos` token\n",
    "\n",
    "After these filters `start_logits` and `end_logits` will both have length of `len(token_to_orig_map)`. So, the best start and idx may have idx values between `(0, len(token_to_orig_map)-1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "\n",
    "_start_logits = start_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "_end_logits = end_logits[attention_mask.bool()][context_start:context_end+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out best idxs so that `star_idx <= end_idx` and `start_logit + end_logit` is max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_best_start_end_idxs(_start_logits, _end_logits):\n",
    "    best_logit = -1e6\n",
    "    best_idxs = None\n",
    "    for start_idx, start_logit in enumerate(_start_logits):\n",
    "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
    "            logit_sum = (start_logit + end_logit).item()\n",
    "            if logit_sum > best_logit:\n",
    "                best_logit = logit_sum\n",
    "                best_idxs = (start_idx, start_idx+end_idx)\n",
    "    return best_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iwillmissyouherein'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(context_tokens[start_idx:end_idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our best start and end idxs we need to shift it by offset so that we can look up original positions form  `token_to_orig_map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_orig_char_start = token_to_orig_map[start_idx+context_start] \n",
    "tok_orig_char_end = token_to_orig_map[end_idx+context_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over original context string char by char to get the answer. This way will be much more robust to retain characteristics of original context and will not be proned to 1-way transformation artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(context) == len(char_to_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def answer_from_orig_context(context, char_to_offset, tok_orig_char_start, tok_orig_char_end):\n",
    "    \"\"\"\n",
    "    Find answer segment char by char from context in \n",
    "    example.context and example.char_to_word_offset\n",
    "    \"\"\"\n",
    "    answer_chars = [char for char, offs_id in zip(context, char_to_offset) \n",
    "                if (offs_id >= tok_orig_char_start) & (offs_id <= tok_orig_char_end)]\n",
    "    predicted_answer = \"\".join(answer_chars).strip()\n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will miss you here in'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_from_orig_context(context, char_to_offset, tok_orig_char_start, tok_orig_char_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "For our datasets we will need the following:\n",
    "\n",
    "#### Examples (Validation/Inference)\n",
    "\n",
    "- `examples[i].context_text`\n",
    "- `examples[i].char_to_word_offset`\n",
    "- `examples[i].answer_text`\n",
    "\n",
    "\n",
    "#### Features (Validation/Inference)\n",
    "\n",
    "- `features[i].token_to_orig_map`\n",
    "\n",
    "#### Dataset Tensors (Validation/Training/Inference)\n",
    "\n",
    "- `input_ids = train_tensors[0][i]`\n",
    "- `attention_mask = train_tensors[1][i]`\n",
    "- `token_type_ids = train_tensors[2][i]`\n",
    "- `start_position = train_tensors[3][i]`\n",
    "- `end_position = train_tensors[4][i]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse] *",
   "language": "python",
   "name": "conda-env-tse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
