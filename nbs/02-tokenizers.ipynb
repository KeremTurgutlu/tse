{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Tokenizers\n",
    "\n",
    ">Exploring tokenizers offered and used by transformers models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers: 0.7.0\n",
      "fastai: 1.0.60\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import tokenizers; print(f\"tokenizers: {tokenizers.__version__}\")\n",
    "import fastai; print(f\"fastai: {fastai.__version__}\")\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "\n",
    "`tokenizers==0.7.0` package. https://twitter.com/moi_anthony/status/1251193880302759938\n",
    "\n",
    "There are many models offered and each of these pretrained models use a specific tokenizer.\n",
    "\n",
    "Python binding [docs](https://github.com/huggingface/tokenizers/blob/71b7830d1b4b633e05cfc2b5271f08a215db2a04/bindings/python/tokenizers/__init__.pyi#L330-L337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/tokenizers_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tokenizers import Tokenizer, AddedToken, pre_tokenizers, decoders, processors\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import BertNormalizer, Lowercase\n",
    "\n",
    "def init_roberta_tokenizer(vocab_file, merges_file, max_length=192, do_lower_case=True):\n",
    "    roberta = Tokenizer(BPE(vocab_file, merges_file))\n",
    "    if do_lower_case: roberta.normalizer = Lowercase() \n",
    "    roberta.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    roberta.decoder = decoders.ByteLevel()\n",
    "\n",
    "    roberta.enable_padding(pad_id=roberta.token_to_id(\"<pad>\"),\n",
    "                           pad_token=\"<pad>\",\n",
    "                           max_length=max_length)\n",
    "\n",
    "    roberta.enable_truncation(max_length=max_length, strategy=\"only_second\")\n",
    "    \n",
    "    roberta.add_special_tokens([\n",
    "        AddedToken(\"<mask>\", lstrip=True),\n",
    "        \"<s>\",\n",
    "        \"</s>\"\n",
    "    ])\n",
    "\n",
    "    roberta.post_processor = processors.RobertaProcessing(\n",
    "        (\"</s>\", roberta.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", roberta.token_to_id(\"<s>\")),\n",
    "    )\n",
    "    \n",
    "    roberta.pad_token_id = roberta.token_to_id(\"<pad>\")\n",
    "    roberta.eos_token_id = roberta.token_to_id(\"</s>\")\n",
    "    roberta.bos_token_id = roberta.token_to_id(\"<s>\")\n",
    "    roberta.unk_token_id = roberta.token_to_id(\"<unk>\")\n",
    "    roberta.mask_token_id = roberta.token_to_id(\"<mask>\")\n",
    "    return roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_roberta_tokenizer(\"../roberta-base/vocab.json\",\n",
    "                                   \"../roberta-base/merges.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3, 50264)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.unk_token_id, tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp = list(tuple(zip(train_df.sentiment, train_df.text)))\n",
    "test_inp = list(tuple(zip(train_df.sentiment, test_df.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neutral', ' I`d have responded, if I were going'),\n",
       " ('negative', ' Sooo SAD I will miss you here in San Diego!!!'),\n",
       " ('negative', 'my boss is bullying me...')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "train_outputs = tokenizer.encode_batch(train_inp)\n",
    "test_outputs = tokenizer.encode_batch(test_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_npads = [sum(array(o.tokens) == '<pad>') for o in train_outputs]\n",
    "# test_npads = [sum(array(o.tokens) == '<pad>') for o in test_outputs]\n",
    "# min(trn_npads), min(test_npads)  == (195, 227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = train_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = output.ids\n",
    "attention_mask = output.attention_mask\n",
    "token_type_ids = output.type_ids\n",
    "offsets = output.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From encoded string we can access the following attributes:\n",
    "\n",
    "- `ids`: token ids\n",
    "- `attention_mask`: binary indicator of what's padded or not\n",
    "- `type_ids`: token type ids for some models, such as BERT\n",
    "- `offsets`: offsets to map tokens back to original string positions\n",
    "- `original_str`: original string\n",
    "- `normalized_str`: original string after normalizer step\n",
    "\n",
    "Also the following methods:\n",
    "\n",
    "-  `truncate()`: to truncate text to a length\n",
    "-  `pad()`: to pad text to a length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4533,  6747,   102,  2939,   821,  3320,    12,   176, 19233,  6315,  1421,   111, 47893,  4483,   428, 13713,\n",
       "        22036,  6315]),\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(toks.ids), toks.attention_mask, array(toks.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer won't have special token handling yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = read_json_as_dict(PRETRAINED_TOK_PATH/'special_tokens_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0\n",
      "</s> 2\n",
      "<unk> 3\n",
      "</s> 2\n",
      "<pad> 1\n",
      "<s> 0\n",
      "<mask> 50264\n"
     ]
    }
   ],
   "source": [
    "for k,v in special_tokens.items(): print(v, tokenizer.token_to_id(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab seems to already have these special tokens since we are using a pretrained tokenizers `vocab.json`. Let's add these as attributes to our `tokenizer` instance for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in special_tokens.items(): setattr(tokenizer, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenizer.bos_token, tokenizer.eos_token, tokenizer.unk_token, tokenizer.sep_token, tokenizer.pad_token, \n",
    "tokenizer.cls_token, tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method 'num_special_tokens_to_add' of 'PostProcessor' objects>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertProcessing.num_special_tokens_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Tokenizers\n",
    "\n",
    "We can alternatively use tokenizer and processors implemented in `transformers` without much hassle. We will use `AutoTokenizer` to load the same pretrained tokenizer. This will handle special tokens for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../tokenizers/roberta-base/tokenizer_config.json'),\n",
       " PosixPath('../tokenizers/roberta-base/special_tokens_map.json'),\n",
       " PosixPath('../tokenizers/roberta-base/config.json'),\n",
       " PosixPath('../tokenizers/roberta-base/merges.txt'),\n",
       " PosixPath('../tokenizers/roberta-base/vocab.json')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_TOK_PATH.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {PRETRAINED_TOK_PATH/'tokenizer_config.json'} {PRETRAINED_TOK_PATH/'config.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(str(PRETRAINED_TOK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> roberta uses gpt-2 tokenizer model - bytelevelbpetokenizer</s></s> which tokenizer model roberta uses?</s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(s, q1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use SQUAD processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "squad_processor = SquadV2Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_DATA_PATH = Path(\"../squad_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21984/21984 [00:00<00:00, 23792.46it/s]\n",
      "100%|██████████| 5496/5496 [00:00<00:00, 29336.36it/s]\n",
      "100%|██████████| 3534/3534 [00:00<00:00, 29409.30it/s]\n"
     ]
    }
   ],
   "source": [
    "train_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'train_squad_data_0.json')\n",
    "valid_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'valid_squad_data_0.json')\n",
    "test_examples = squad_processor.get_dev_examples(SQUAD_DATA_PATH, 'test_squad_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_examples[0].question_text\n",
    "answer = train_examples[0].answer_text\n",
    "context = train_examples[0].context_text \n",
    "char_to_offset = array(train_examples[0].char_to_word_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " 'Sooo SAD',\n",
       " ' Sooo SAD I will miss you here in San Diego!!!',\n",
       " array([-1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,\n",
       "         6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, answer, context, char_to_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MAX_SEQ_LEN = 104\n",
    "MAX_QUERY_LEN = 5\n",
    "DOC_STRIDE = 200 # useful for LM modeling\n",
    "def get_squad_dataset(examples, tokenizer, is_training):\n",
    "    return squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        doc_stride=DOC_STRIDE,\n",
    "        max_seq_length=MAX_SEQ_LEN,\n",
    "        max_query_length=MAX_QUERY_LEN,\n",
    "        is_training=is_training,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=defaults.cpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 21984/21984 [00:06<00:00, 3339.75it/s]\n",
      "add example index and unique id: 100%|██████████| 21984/21984 [00:00<00:00, 838891.33it/s]\n",
      "convert squad examples to features: 100%|██████████| 5496/5496 [00:01<00:00, 3617.62it/s]\n",
      "add example index and unique id: 100%|██████████| 5496/5496 [00:00<00:00, 830968.41it/s]\n",
      "convert squad examples to features: 100%|██████████| 3534/3534 [00:00<00:00, 4321.36it/s]\n",
      "add example index and unique id: 100%|██████████| 3534/3534 [00:00<00:00, 723374.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_dataset = get_squad_dataset(train_examples, tokenizer, True)\n",
    "valid_features, valid_dataset = get_squad_dataset(valid_examples, tokenizer, True)\n",
    "test_features, test_dataset = get_squad_dataset(test_examples, tokenizer, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that we can extract `input_ids`, `attention_mask`, `token_type_ids` (not used by Roberta) for modeling. `token_to_orig_map` can be used for mapping tokens back to original string. Note that this dictionary starts with `4th idx` token since first 4 are our question `['<s>', 'negative', '</s>', '</s>']` tokens, also the last one is not used since it is eos token `'</s>'`. We need take this into account when mapping back from tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " {4: 0,\n",
       "  5: 0,\n",
       "  6: 1,\n",
       "  7: 1,\n",
       "  8: 2,\n",
       "  9: 3,\n",
       "  10: 4,\n",
       "  11: 5,\n",
       "  12: 6,\n",
       "  13: 7,\n",
       "  14: 8,\n",
       "  15: 9,\n",
       "  16: 9,\n",
       "  17: 9})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_orig_map = train_features[0].token_to_orig_map\n",
    "context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "len(token_to_orig_map), token_to_orig_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,\n",
       " array(['<s>', 'negative', '</s>', '</s>', 'so', 'oo', 's', 'ad', 'i', 'will', 'miss', 'you', 'here', 'in', 'san',\n",
       "        'die', 'go', '!!!', '</s>'], dtype='<U8'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = array(train_features[0].tokens)\n",
    "len(tokens), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " array(['so', 'oo', 's', 'ad', 'i', 'will', 'miss', 'you', 'here', 'in', 'san', 'die', 'go', '!!!'], dtype='<U8'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[context_start:context_end+1]), tokens[context_start:context_end+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end predictions may contain indexes from `(0, MAX_SEQ_LEN)` so we will need some post processing to map back to original string for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = train_dataset.tensors\n",
    "\n",
    "input_ids = train_tensors[0][0]\n",
    "attention_mask = train_tensors[1][0]\n",
    "token_type_ids = train_tensors[2][0]\n",
    "start_position = train_tensors[3][0].item()\n",
    "end_position = train_tensors[4][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([104]), torch.Size([104]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "start_logits = torch.randn_like(input_ids, dtype=torch.float)\n",
    "end_logits = torch.randn_like(input_ids, dtype=torch.float)\n",
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter `start_logits` and `end_logits` before finding the best start and end idxs:\n",
    "\n",
    "- Filter by `attention_mask` to exclude padding\n",
    "- Filter by question tokens [4:] first 4 tokens in our case, which can be also obtained from `min() and max()` keys of `token_to_orig_map` for variable question lengths\n",
    "- Exclude final token [:-1] which is the `eos` token\n",
    "\n",
    "After these filters `start_logits` and `end_logits` will both have length of `len(token_to_orig_map)`. So, the best start and idx may have idx values between `(0, len(token_to_orig_map)-1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "\n",
    "_start_logits = start_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "_end_logits = end_logits[attention_mask.bool()][context_start:context_end+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out best idxs so that `star_idx <= end_idx` and `start_logit + end_logit` is max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_best_start_end_idxs(_start_logits, _end_logits):\n",
    "    best_logit = -1e6\n",
    "    best_idxs = None\n",
    "    for start_idx, start_logit in enumerate(_start_logits):\n",
    "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
    "            logit_sum = (start_logit + end_logit).item()\n",
    "            if logit_sum > best_logit:\n",
    "                best_logit = logit_sum\n",
    "                best_idxs = (start_idx, start_idx+end_idx)\n",
    "    return best_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iwillmissyouherein'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(context_tokens[start_idx:end_idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our best start and end idxs we need to shift it by offset so that we can look up original positions form  `token_to_orig_map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_orig_char_start = token_to_orig_map[start_idx+context_start] \n",
    "tok_orig_char_end = token_to_orig_map[end_idx+context_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over original context string char by char to get the answer. This way will be much more robust to retain characteristics of original context and will not be proned to 1-way transformation artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(context) == len(char_to_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def answer_from_orig_context(context, char_to_offset, tok_orig_char_start, tok_orig_char_end):\n",
    "    \"\"\"\n",
    "    Find answer segment char by char from context in \n",
    "    example.context and example.char_to_word_offset\n",
    "    \"\"\"\n",
    "    answer_chars = [char for char, offs_id in zip(context, char_to_offset) \n",
    "                if (offs_id >= tok_orig_char_start) & (offs_id <= tok_orig_char_end)]\n",
    "    predicted_answer = \"\".join(answer_chars).strip()\n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will miss you here in'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_from_orig_context(context, char_to_offset, tok_orig_char_start, tok_orig_char_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "For our datasets we will need the following:\n",
    "\n",
    "#### Examples (Validation/Inference)\n",
    "\n",
    "- `examples[i].context_text`\n",
    "- `examples[i].char_to_word_offset`\n",
    "- `examples[i].answer_text`\n",
    "\n",
    "\n",
    "#### Features (Validation/Inference)\n",
    "\n",
    "- `features[i].token_to_orig_map`\n",
    "\n",
    "#### Dataset Tensors (Validation/Training/Inference)\n",
    "\n",
    "- `input_ids = train_tensors[0][i]`\n",
    "- `attention_mask = train_tensors[1][i]`\n",
    "- `token_type_ids = train_tensors[2][i]`\n",
    "- `start_position = train_tensors[3][i]`\n",
    "- `end_position = train_tensors[4][i]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n",
      "Converted 04-models.ipynb.\n",
      "Converted post-process.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse]",
   "language": "python",
   "name": "conda-env-tse-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
