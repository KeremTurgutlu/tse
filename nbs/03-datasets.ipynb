{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.text import *\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tse.squad_utils import *\n",
    "from tse.tokenizers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataAugmentor\n",
    "\n",
    "> Dataset and Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SQUAD_DATA_PATH = Path(\"../squad_data/\")\n",
    "PRETRAINED_TOK_PATH = Path(\"../tokenizers/roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(str(PRETRAINED_TOK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version - don't export\n",
    "class SQUAD_Dataset(Dataset):\n",
    "    def __init__(self, dataset_tensors, examples, features, is_training=True):\n",
    "        self.dataset_tensors = dataset_tensors\n",
    "        self.examples = examples\n",
    "        self.features = features\n",
    "        self.is_training = is_training\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'fastai requires (xb, yb) to return'\n",
    "\n",
    "        input_ids = self.dataset_tensors[0][idx]\n",
    "        attention_mask = self.dataset_tensors[1][idx]\n",
    "        token_type_ids = self.dataset_tensors[2][idx]\n",
    "        xb = (input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        if self.is_training:\n",
    "            start_positions = self.dataset_tensors[3][idx]\n",
    "            end_positions = self.dataset_tensors[4][idx]\n",
    "        yb = (start_positions, end_positions)\n",
    "\n",
    "        return xb, yb\n",
    "\n",
    "    def __len__(self): return len(self.dataset_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version - don't export\n",
    "def get_fold_ds(foldnum, tokenizer):\n",
    "    data_dir = \"/kaggle/working/squad_data\"\n",
    "    train_filename = f\"train_squad_data_{foldnum}.json\"\n",
    "    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n",
    "    test_filename = \"test_squad_data.json\"\n",
    "    \n",
    "    # examples\n",
    "    train_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'train_squad_data_0.json')\n",
    "    valid_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'valid_squad_data_0.json')\n",
    "    test_examples = squad_processor.get_dev_examples(SQUAD_DATA_PATH, 'test_squad_data.json')\n",
    "\n",
    "    # features and tensors\n",
    "    train_features, train_dataset = get_squad_dataset(train_examples, tokenizer, True)\n",
    "    valid_features, valid_dataset = get_squad_dataset(valid_examples, tokenizer, True)\n",
    "    test_features, test_dataset = get_squad_dataset(test_examples, tokenizer, False)\n",
    "    train_dataset_tensors = train_dataset.tensors\n",
    "    valid_dataset_tensors = valid_dataset.tensors\n",
    "    test_dataset_tensors = test_dataset.tensors\n",
    "    \n",
    "    # create pytorch dataset\n",
    "    train_ds = SQUAD_Dataset(train_dataset_tensors, train_examples, train_features)\n",
    "    valid_ds = SQUAD_Dataset(valid_dataset_tensors, valid_examples, valid_features)\n",
    "    test_ds = SQUAD_Dataset(test_dataset_tensors, test_examples, test_features, False)\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Augmentations to be done on training set\n",
    "\n",
    "- 1a) Random split - always keep full answer\n",
    "- 1b) Random split - can split from anwhere\n",
    "- 2) Randomly mask tokens \n",
    "- 3) Randomly mask tokens within the answer context\n",
    "- 4) Left - Right Flip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21984/21984 [00:00<00:00, 24434.08it/s]\n",
      "100%|██████████| 5496/5496 [00:00<00:00, 24844.50it/s]\n",
      "100%|██████████| 3534/3534 [00:00<00:00, 26853.10it/s]\n",
      "convert squad examples to features: 100%|██████████| 21984/21984 [00:08<00:00, 2743.49it/s]\n",
      "add example index and unique id: 100%|██████████| 21984/21984 [00:00<00:00, 788665.19it/s]\n",
      "convert squad examples to features: 100%|██████████| 5496/5496 [00:02<00:00, 2502.20it/s]\n",
      "add example index and unique id: 100%|██████████| 5496/5496 [00:00<00:00, 753502.26it/s]\n",
      "convert squad examples to features: 100%|██████████| 3534/3534 [00:01<00:00, 2192.57it/s]\n",
      "add example index and unique id: 100%|██████████| 3534/3534 [00:00<00:00, 608234.32it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_fold_ds(0, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text = train_ds.examples[i].answer_text\n",
    "context_text = train_ds.examples[i].context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = train_ds.examples[i]\n",
    "features0 = train_ds.features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = train_ds[i][0][0]\n",
    "attention_mask = train_ds[i][0][1]\n",
    "token_type_ids = train_ds[i][0][2]\n",
    "start_position = train_ds[i][1][0]\n",
    "end_position = train_ds[i][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_orig_map = train_ds.features[i].token_to_orig_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSEDataAugmentor\n",
    "\n",
    "#### 1) Random Left Right Truncate\n",
    "\n",
    "```\n",
    "-> tok3 anstok anstok anstok tok7 (rand left and right idxs)\n",
    "-> tok3 anstok anstok anstok tok7 tok8 (rand left idx)\n",
    "-> Tok1 tok2 tok3 anstok anstok anstok tok7 (rand right idx)\n",
    "```\n",
    "\n",
    "\n",
    "#### 2) Random Mask\n",
    "\n",
    "```\n",
    "-> Tok1 tok2 <MASK> anstok anstok anstok tok7 <MASK>\n",
    "-> Tok1 tok2 <UNK> anstok anstok anstok tok7 <UNK>\n",
    "```\n",
    "\n",
    "#### 3) TODO: Random Left Right Flip\n",
    "\n",
    "\n",
    "- Should be at word level not token: `\" \".join(sent.split()[::-1])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSEDataAugmentor():\n",
    "\n",
    "    def __init__(self, tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map): \n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        \n",
    "        # initial answer start and end positions\n",
    "        self.ans_start_pos, self.ans_end_pos = start_position.item(), end_position.item()\n",
    "                \n",
    "        # initial context start and end positions\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.context_start_pos, self.context_end_pos = min(token_to_orig_map), max(token_to_orig_map)\n",
    "\n",
    "        \n",
    "    \n",
    "    # left and right indexes excluding answer tokens and eos token\n",
    "    @property\n",
    "    def left_idxs(self): return np.arange(self.context_start_pos, self.ans_start_pos)\n",
    "    \n",
    "    @property\n",
    "    def right_idxs(self): return np.arange(self.ans_end_pos+1, self.context_end_pos+1)\n",
    "    \n",
    "    @property\n",
    "    def left_right_idxs(self): return np.concatenate([self.left_idxs, self.right_idxs])\n",
    "    \n",
    "    @property\n",
    "    def rand_left_idx(self): return np.random.choice(self.left_idxs) if self.left_idxs.size > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def rand_right_idx(self): return np.random.choice(self.right_idxs) if self.right_idxs.size > 0 else None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def right_truncate(self, right_idx):\n",
    "        \"\"\"\n",
    "        Truncate context from random right index to beginning, answer pos doesn't change\n",
    "        Note: token_type_ids NotImplemented\n",
    "        \"\"\"\n",
    "        if not right_idx: raise Exception(\"Right index can't be None\")\n",
    "        \n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        nopad_input_ids = new_input_ids[self.attention_mask.bool()]\n",
    "        \n",
    "        # truncate from right idx to beginning - add eos_token_id to end\n",
    "        truncated = torch.cat([nopad_input_ids[:right_idx+1], tensor([self.tokenizer.eos_token_id])])\n",
    "        \n",
    "        # pad new context until size are equal\n",
    "        # replace original input context with new\n",
    "        n_pad = len(nopad_input_ids) - len(truncated)\n",
    "        new_context = F.pad(truncated, (0,n_pad), value=self.tokenizer.pad_token_id)\n",
    "        new_input_ids[:self.context_end_pos+2] = new_context\n",
    "        \n",
    "        \n",
    "        # find new attention mask, update new context end position (exclude eos token)\n",
    "        # Note: context start doesn't change since we don't manipulate question\n",
    "        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n",
    "        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1 \n",
    "        self.context_end_pos = new_context_end_pos\n",
    "        \n",
    "        # update input_ids and attention_masks\n",
    "        self.input_ids = new_input_ids\n",
    "        self.attention_mask = new_attention_mask\n",
    "        \n",
    "        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n",
    "\n",
    "    def random_right_truncate(self):\n",
    "        right_idx = self.rand_right_idx\n",
    "        if right_idx: self.right_truncate(right_idx)\n",
    "    \n",
    "    \n",
    "    def left_truncate(self, left_idx):\n",
    "        \"\"\"\n",
    "        Truncate context from random left index to end, answer pos changes too\n",
    "        Note: token_type_ids NotImplemented\n",
    "        \"\"\"\n",
    "        \n",
    "        if not left_idx: raise Exception(\"Left index can't be None\")\n",
    "        \n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        \n",
    "        # pad new context until size are equal\n",
    "        # replace original input context with new\n",
    "\n",
    "        n_pad = len(new_input_ids[self.context_start_pos:]) - len(new_input_ids[left_idx:])\n",
    "        \n",
    "        new_context = F.pad(new_input_ids[left_idx:], (0,n_pad), value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        new_input_ids[self.context_start_pos:] = new_context\n",
    "        \n",
    "                \n",
    "        # find new attention mask, update new context end position (exclude eos token)\n",
    "        # Note: context start doesn't change since we don't manipulate question\n",
    "        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n",
    "        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1\n",
    "        self.context_end_pos = new_context_end_pos\n",
    "        \n",
    "        # find new answer start and end positions\n",
    "        # update new answer start and end positions\n",
    "        ans_shift = left_idx - self.context_start_pos\n",
    "        self.ans_start_pos, self.ans_end_pos = self.ans_start_pos-ans_shift, self.ans_end_pos-ans_shift\n",
    "        \n",
    "        \n",
    "        # update input_ids and attention_masks\n",
    "        self.input_ids = new_input_ids\n",
    "        self.attention_mask = new_attention_mask\n",
    "        \n",
    "        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n",
    "        \n",
    "    def random_left_truncate(self):\n",
    "        left_idx = self.rand_left_idx\n",
    "        if left_idx: self.left_truncate(left_idx)\n",
    "        \n",
    "        \n",
    "    def replace_with_mask(self, idxs_to_mask):\n",
    "        \"\"\"\n",
    "        Replace given input ids with tokenizer.mask_token_id\n",
    "        \"\"\"\n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        new_input_ids[idxs_to_mask] = tensor([tokenizer.mask_token_id]*len(idxs_to_mask))\n",
    "        self.input_ids = new_input_ids\n",
    "\n",
    "        \n",
    "    def random_replace_with_mask(self, mask_p=0.2):\n",
    "        \"\"\"\n",
    "        mask_p: Proportion of tokens to replace with mask token id\n",
    "        \"\"\"\n",
    "        idxs_to_mask = np.random.choice(self.left_right_idxs, int(len(self.left_right_idxs)*mask_p))\n",
    "        if idxs_to_mask.size > 0: self.replace_with_mask(idxs_to_mask)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 33407,     2,     2,  2527,  3036,    29,   625,   118,  6677,\n",
       "        17745,  6968, 10859,   179, 14832, 18554,  2977, 16506,     2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[attention_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4), tensor(7))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position, end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sooo SAD', ' Sooo SAD I will miss you here in San Diego!!!', 4, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_text, context_text, start_position.item(), end_position.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> negative </s> </s> so oo s ad i will miss you here in san die go !!! </s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokenizer.convert_ids_to_tokens(input_ids[attention_mask.bool()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so oo s ad'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokenizer.convert_ids_to_tokens(input_ids[start_position.item(): end_position.item()+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo right truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> negative </s> </s> so oo s ad i will miss you </s>\n",
      "\n",
      "so oo s ad\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map)\n",
    "da.random_right_truncate()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo left truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> negative </s> </s> so oo s ad i will miss you here in san die go !!! </s>\n",
      "\n",
      "so oo s ad\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map)\n",
    "da.random_left_truncate()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo replace with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> negative </s> </s> so oo s ad i <mask> miss you here in san die <mask> !!! </s>\n",
      "\n",
      "so oo s ad\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map)\n",
    "da.random_replace_with_mask(0.2)\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.left_idxs, da.right_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> negative </s> </s> so oo s ad i <mask> miss you here </s>\n",
      "\n",
      "so oo s ad\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position, token_to_orig_map)\n",
    "\n",
    "da.random_left_truncate()\n",
    "da.random_right_truncate()\n",
    "da.random_replace_with_mask(0.3)\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQUAD_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_left_truncate': {'p': 0.3},\n",
       " 'random_right_truncate': {'p': 0.3},\n",
       " 'random_replace_with_mask': {'p': 0.3, 'mask_p': 0.2}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_tfms = {}\n",
    "do_tfms[\"random_left_truncate\"] = {\"p\":0.3}\n",
    "do_tfms[\"random_right_truncate\"] = {\"p\":0.3}\n",
    "do_tfms[\"random_replace_with_mask\"] = {\"p\":0.3, \"mask_p\":0.2}\n",
    "do_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SQUAD_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset_tensors, examples, features, is_training=True, do_tfms=None):\n",
    "        self.dataset_tensors = dataset_tensors\n",
    "        self.examples = examples\n",
    "        self.features = features\n",
    "        self.is_training = is_training\n",
    "        self.tokenizer = tokenizer\n",
    "        self.do_tfms = do_tfms\n",
    "                \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        'fastai requires (xb, yb) to return'\n",
    "        \n",
    "        input_ids = self.dataset_tensors[0][idx]\n",
    "        attention_mask = self.dataset_tensors[1][idx]\n",
    "        token_type_ids = self.dataset_tensors[2][idx]\n",
    "        if self.is_training: \n",
    "            start_position = self.dataset_tensors[3][idx]\n",
    "            end_position = self.dataset_tensors[4][idx]\n",
    "            \n",
    "            if self.do_tfms:\n",
    "                token_to_orig_map = self.features[idx].token_to_orig_map\n",
    "                \n",
    "                augmentor = TSEDataAugmentor(self.tokenizer,\n",
    "                                             input_ids,\n",
    "                                             attention_mask,\n",
    "                                             start_position, end_position,\n",
    "                                             token_to_orig_map)\n",
    "\n",
    "                if np.random.uniform() < self.do_tfms[\"random_left_truncate\"][\"p\"]:\n",
    "                    augmentor.random_left_truncate()\n",
    "                if np.random.uniform() < self.do_tfms[\"random_right_truncate\"][\"p\"]:\n",
    "                    augmentor.random_right_truncate()\n",
    "                if np.random.uniform() < self.do_tfms[\"random_replace_with_mask\"][\"p\"]:\n",
    "                    augmentor.random_replace_with_mask(self.do_tfms[\"random_replace_with_mask\"][\"mask_p\"])\n",
    "\n",
    "                input_ids = augmentor.input_ids\n",
    "                attention_mask = augmentor.attention_mask\n",
    "                start_position, end_position = tensor(augmentor.ans_start_pos), tensor(augmentor.ans_end_pos)\n",
    "                \n",
    "            \n",
    "        xb = (input_ids, attention_mask, token_type_ids)\n",
    "        if self.is_training: yb = (start_position, end_position)\n",
    "        else: yb = 0\n",
    "        \n",
    "        return xb, yb\n",
    "    \n",
    "    def __len__(self): return len(self.dataset_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_fold_ds(foldnum, tokenizer):\n",
    "    data_dir = \"/kaggle/working/squad_data\"\n",
    "    train_filename = f\"train_squad_data_{foldnum}.json\"\n",
    "    valid_filename = f\"valid_squad_data_{foldnum}.json\"\n",
    "    test_filename = \"test_squad_data.json\"\n",
    "    \n",
    "    # examples\n",
    "    train_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'train_squad_data_0.json')\n",
    "    valid_examples = squad_processor.get_train_examples(SQUAD_DATA_PATH, 'valid_squad_data_0.json')\n",
    "    test_examples = squad_processor.get_dev_examples(SQUAD_DATA_PATH, 'test_squad_data.json')\n",
    "\n",
    "    # features and tensors\n",
    "    train_features, train_dataset = get_squad_dataset(train_examples, tokenizer, True)\n",
    "    valid_features, valid_dataset = get_squad_dataset(valid_examples, tokenizer, True)\n",
    "    test_features, test_dataset = get_squad_dataset(test_examples, tokenizer, False)\n",
    "    train_dataset_tensors = train_dataset.tensors\n",
    "    valid_dataset_tensors = valid_dataset.tensors\n",
    "    test_dataset_tensors = test_dataset.tensors\n",
    "    \n",
    "    # create pytorch dataset\n",
    "    do_tfms = {}\n",
    "    do_tfms[\"random_left_truncate\"] = {\"p\":0.3}\n",
    "    do_tfms[\"random_right_truncate\"] = {\"p\":0.3}\n",
    "    do_tfms[\"random_replace_with_mask\"] = {\"p\":0.3, \"mask_p\":0.3}\n",
    "\n",
    "    train_ds = SQUAD_Dataset(tokenizer, train_dataset_tensors, train_examples, train_features, True, do_tfms)\n",
    "    valid_ds = SQUAD_Dataset(tokenizer, valid_dataset_tensors, valid_examples, valid_features, True)\n",
    "    test_ds = SQUAD_Dataset(tokenizer, test_dataset_tensors, test_examples, test_features, False)\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21984/21984 [00:01<00:00, 19703.62it/s]\n",
      "100%|██████████| 5496/5496 [00:00<00:00, 27330.81it/s]\n",
      "100%|██████████| 3534/3534 [00:00<00:00, 27846.25it/s]\n",
      "convert squad examples to features: 100%|██████████| 21984/21984 [00:08<00:00, 2731.55it/s]\n",
      "add example index and unique id: 100%|██████████| 21984/21984 [00:00<00:00, 907528.12it/s]\n",
      "convert squad examples to features: 100%|██████████| 5496/5496 [00:02<00:00, 2698.70it/s]\n",
      "add example index and unique id: 100%|██████████| 5496/5496 [00:00<00:00, 708156.02it/s]\n",
      "convert squad examples to features: 100%|██████████| 3534/3534 [00:01<00:00, 3105.58it/s]\n",
      "add example index and unique id: 100%|██████████| 3534/3534 [00:00<00:00, 729246.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, test_ds = get_fold_ds(0, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow... u just became cooler.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.examples[i].answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> positive </s> </s> j ourney !? wow ... u just bec ame cool er . he he ... ( is that p ossible ! ?) </s>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_ids, attention_mask, token_type_ids), (start_positions, end_positions) = train_ds[i]\n",
    "\n",
    "\" \".join(train_ds.tokenizer.convert_ids_to_tokens(input_ids[attention_mask.bool()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict_answer_text`\n",
    "\n",
    "TODO: Migrate to proper notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict_answer_text(start_logits, end_logits, attention_mask,\n",
    "                        context_text, char_to_word_offset, token_to_orig_map): \n",
    "    \"Find best answer from context\"\n",
    "    # find best start and end\n",
    "    context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "    truncated_start_logits = start_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "    truncated_end_logits = end_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "    best_start_idx, best_end_idx = find_best_start_end_idxs(truncated_start_logits, truncated_end_logits)\n",
    "    \n",
    "    # generate answer\n",
    "    tok_orig_char_start = token_to_orig_map[best_start_idx+context_start] \n",
    "    tok_orig_char_end = token_to_orig_map[best_end_idx+context_start]\n",
    "    return answer_from_orig_context(context_text, char_to_word_offset, tok_orig_char_start, tok_orig_char_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edings for the baby are fun when he is a'"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_text(start_logits, end_logits, attention_mask, \n",
    "                   context_text, char_to_word_offset, token_to_orig_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse] *",
   "language": "python",
   "name": "conda-env-tse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
