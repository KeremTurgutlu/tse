{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers: 0.7.0\n",
      "fastai: 1.0.60\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from fastai.text import *\n",
    "from tse.tokenizers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Inputs for Q/A\n",
    "\n",
    "\n",
    "Following for each input for training is needed:\n",
    "\n",
    "`input_ids`, `attention_mask`, `token_type_ids`, `offsets`, `answer_text`, `start_tok_idx`, `end_tok_idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_roberta_tokenizer(\"../roberta-base/vocab.json\",\n",
    "                                   \"../roberta-base/merges.txt\", max_length=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and strip data\n",
    "train_df = pd.read_csv(\"../data/train.csv\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "train_df.selected_text = train_df.selected_text.apply(lambda s: s.strip())\n",
    "train_df.text = train_df.text.apply(lambda s: s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on th...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861  Sons of ****, why couldn`t they put them on th...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_start_end_idxs(context, answer):\n",
    "    \"Get string start and end char for answer span\"\n",
    "    len_a = len(answer)\n",
    "    for i, _ in enumerate(context):\n",
    "        if context[i:i+len_a] == answer: \n",
    "            start_idx, end_idx = i, i+len_a-1\n",
    "            return start_idx, end_idx\n",
    "    raise Exception(\"No overlapping segment found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_start_end_tok_idxs(offsets, start_idx, end_idx):\n",
    "    \"Generate target from tokens - first 4 tokens belong to question\"\n",
    "    start_tok_idx, end_tok_idx = None, None\n",
    "    for tok_idx, off in enumerate(offsets[4:]):\n",
    "        if (off[0] <= start_idx) & (off[1] > start_idx): start_tok_idx = tok_idx + 4\n",
    "        if (off[0] <= end_idx) & (off[1] > end_idx): end_tok_idx = tok_idx + 4\n",
    "    return (start_tok_idx, end_tok_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_stxt, trn_txt, trn_sent = train_df.selected_text.values, train_df.text.values, train_df.sentiment\n",
    "test_txt, test_sent = test_df.text.values, test_df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_input = list(tuple(zip(trn_sent, trn_txt)))\n",
    "test_tok_input = list(tuple(zip(test_sent, test_txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode batch\n",
    "train_outputs = tokenizer.encode_batch(train_tok_input)\n",
    "test_outputs = tokenizer.encode_batch(test_tok_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_idxs = [get_start_end_idxs(s1,s2) for (s1,s2) in zip(trn_txt, trn_stxt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QAInputGenerator:\n",
    "    def __init__(self, contexts, questions, answers=None, tokenizer=None):\n",
    "        self.contexts, self.questions, self.answers = contexts, questions, answers\n",
    "        self.outputs = tokenizer.encode_batch(list(tuple(zip(questions, contexts))))\n",
    "        if self.answers is not None:\n",
    "            self.start_end_idxs = [get_start_end_idxs(s1,s2) for (s1,s2) in zip(self.contexts, self.answers)]\n",
    "            \n",
    "            \n",
    "    @classmethod\n",
    "    def from_df(cls, df, ctx_col='text', q_col='sentiment', ans_col='selected_text', \n",
    "                is_test=False, tokenizer=None):\n",
    "        contexts = df[ctx_col].values\n",
    "        questions = df[q_col].values\n",
    "        answers = None if is_test else df[ans_col].values\n",
    "        return cls(contexts, questions, answers, tokenizer)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        input_ids = array(self.outputs[i].ids)\n",
    "        attention_mask = array(self.outputs[i].attention_mask)\n",
    "        offsets = array(self.outputs[i].offsets)\n",
    "        tokens = array(self.outputs[i].tokens)\n",
    "        res = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"offsets\": offsets, \n",
    "              \"tokens\": tokens, \"context_text\": self.contexts[i]}\n",
    "        \n",
    "        if self.answers is not None:\n",
    "            answer_text = self.answers[i]\n",
    "            start_tok_idx, end_tok_idx = get_start_end_tok_idxs(offsets, *self.start_end_idxs[i])\n",
    "            res[\"answer_text\"] = answer_text\n",
    "            res[\"start_end_tok_idxs\"] = (start_tok_idx, end_tok_idx)\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def __len__(self): return len(self.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = QAInputGenerator.from_df(train_df, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = QAInputGenerator.from_df(test_df, is_test=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'offsets', 'tokens', 'context_text', 'answer_text', 'start_end_tok_idxs'])\n",
      "['h' 'Ġthank' 'Ġyou']\n",
      "h thank you\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(range(len(train_inputs)))\n",
    "print(train_inputs[i].keys())\n",
    "print(train_inputs[i]['tokens'][train_inputs[i]['start_end_tok_idxs'][0]:train_inputs[i]['start_end_tok_idxs'][1]+1])\n",
    "print(train_inputs[i]['answer_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'offsets', 'tokens', 'context_text'])\n",
      "['<s>' 'Ġpositive' '</s>' '</s>' 'Ġyou' 'Ġlooked' 'Ġabsolutely' 'Ġbeautiful' 'Ġand' 'Ġelegant' '.' '</s>']\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(range(len(test_inputs)))\n",
    "print(test_inputs[i].keys())\n",
    "print(test_inputs[i]['tokens'][test_inputs[i]['attention_mask'].astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = list(train_inputs)\n",
    "test_inputs = list(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 3534)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSEDataAugmentor\n",
    "\n",
    "#### 1) Random Left Right Truncate\n",
    "\n",
    "```\n",
    "-> tok3 anstok anstok anstok tok7 (rand left and right idxs)\n",
    "-> tok3 anstok anstok anstok tok7 tok8 (rand left idx)\n",
    "-> Tok1 tok2 tok3 anstok anstok anstok tok7 (rand right idx)\n",
    "```\n",
    "\n",
    "\n",
    "#### 2) Random Mask\n",
    "\n",
    "```\n",
    "-> Tok1 tok2 <MASK> anstok anstok anstok tok7 <MASK>\n",
    "-> Tok1 tok2 <UNK> anstok anstok anstok tok7 <UNK>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSEDataAugmentor:\n",
    "    def __init__(self, tokenizer, input_ids, attention_mask, start_position, end_position): \n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        \n",
    "        # initial answer start and end positions\n",
    "        self.ans_start_pos, self.ans_end_pos = start_position.item(), end_position.item()\n",
    "                        \n",
    "        # context token start and end excluding bos - eos tokens\n",
    "        self.context_start_pos = 4\n",
    "        self.context_end_pos = torch.where(attention_mask)[0][-1].item() - 1\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    # left and right indexes excluding answer tokens and eos token\n",
    "    @property\n",
    "    def left_idxs(self): return np.arange(self.context_start_pos, self.ans_start_pos)\n",
    "    \n",
    "    @property\n",
    "    def right_idxs(self): return np.arange(self.ans_end_pos+1, self.context_end_pos+1)\n",
    "    \n",
    "    @property\n",
    "    def left_right_idxs(self): return np.concatenate([self.left_idxs, self.right_idxs])\n",
    "    \n",
    "    @property\n",
    "    def rand_left_idx(self): return np.random.choice(self.left_idxs) if self.left_idxs.size > 0 else None\n",
    "    \n",
    "    @property\n",
    "    def rand_right_idx(self): return np.random.choice(self.right_idxs) if self.right_idxs.size > 0 else None\n",
    "        \n",
    "    \n",
    "    \n",
    "    def right_truncate(self, right_idx):\n",
    "        \"\"\"\n",
    "        Truncate context from random right index to beginning, answer pos doesn't change\n",
    "        Note: token_type_ids NotImplemented\n",
    "        \"\"\"\n",
    "        if not right_idx: raise Exception(\"Right index can't be None\")\n",
    "        \n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        nopad_input_ids = new_input_ids[self.attention_mask.bool()]\n",
    "        \n",
    "        # truncate from right idx to beginning - add eos_token_id to end\n",
    "        truncated = torch.cat([nopad_input_ids[:right_idx+1], tensor([self.tokenizer.eos_token_id])])\n",
    "        \n",
    "        # pad new context until size are equal\n",
    "        # replace original input context with new\n",
    "        n_pad = len(nopad_input_ids) - len(truncated)\n",
    "        new_context = F.pad(truncated, (0,n_pad), value=self.tokenizer.pad_token_id)\n",
    "        new_input_ids[:self.context_end_pos+2] = new_context\n",
    "        \n",
    "        \n",
    "        # find new attention mask, update new context end position (exclude eos token)\n",
    "        # Note: context start doesn't change since we don't manipulate question\n",
    "        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n",
    "        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1 \n",
    "        self.context_end_pos = new_context_end_pos\n",
    "        \n",
    "        # update input_ids and attention_masks\n",
    "        self.input_ids = new_input_ids\n",
    "        self.attention_mask = new_attention_mask\n",
    "        \n",
    "        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n",
    "\n",
    "    def random_right_truncate(self):\n",
    "        right_idx = self.rand_right_idx\n",
    "        if right_idx: self.right_truncate(right_idx)\n",
    "    \n",
    "    \n",
    "    def left_truncate(self, left_idx):\n",
    "        \"\"\"\n",
    "        Truncate context from random left index to end, answer pos changes too\n",
    "        Note: token_type_ids NotImplemented\n",
    "        \"\"\"\n",
    "        \n",
    "        if not left_idx: raise Exception(\"Left index can't be None\")\n",
    "        \n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        \n",
    "        # pad new context until size are equal\n",
    "        # replace original input context with new\n",
    "\n",
    "        n_pad = len(new_input_ids[self.context_start_pos:]) - len(new_input_ids[left_idx:])\n",
    "        \n",
    "        new_context = F.pad(new_input_ids[left_idx:], (0,n_pad), value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        new_input_ids[self.context_start_pos:] = new_context\n",
    "        \n",
    "                \n",
    "        # find new attention mask, update new context end position (exclude eos token)\n",
    "        # Note: context start doesn't change since we don't manipulate question\n",
    "        new_attention_mask = tensor([1 if i != 1 else 0 for i in new_input_ids])\n",
    "        new_context_end_pos = torch.where(new_attention_mask)[0][-1].item() - 1\n",
    "        self.context_end_pos = new_context_end_pos\n",
    "        \n",
    "        # find new answer start and end positions\n",
    "        # update new answer start and end positions\n",
    "        ans_shift = left_idx - self.context_start_pos\n",
    "        self.ans_start_pos, self.ans_end_pos = self.ans_start_pos-ans_shift, self.ans_end_pos-ans_shift\n",
    "        \n",
    "        \n",
    "        # update input_ids and attention_masks\n",
    "        self.input_ids = new_input_ids\n",
    "        self.attention_mask = new_attention_mask\n",
    "        \n",
    "        return self.input_ids, self.attention_mask, (tensor(self.ans_start_pos), tensor(self.ans_end_pos))\n",
    "        \n",
    "    def random_left_truncate(self):\n",
    "        left_idx = self.rand_left_idx\n",
    "        if left_idx: self.left_truncate(left_idx)\n",
    "        \n",
    "        \n",
    "    def replace_with_mask(self, idxs_to_mask):\n",
    "        \"\"\"\n",
    "        Replace given input ids with tokenizer.mask_token_id\n",
    "        \"\"\"\n",
    "        # clone for debugging\n",
    "        new_input_ids = self.input_ids.clone()\n",
    "        new_input_ids[idxs_to_mask] = tensor([self.tokenizer.mask_token_id]*len(idxs_to_mask))\n",
    "        self.input_ids = new_input_ids\n",
    "\n",
    "        \n",
    "    def random_replace_with_mask(self, mask_p=0.2):\n",
    "        \"\"\"\n",
    "        mask_p: Proportion of tokens to replace with mask token id\n",
    "        \"\"\"\n",
    "        idxs_to_mask = np.random.choice(self.left_right_idxs, int(len(self.left_right_idxs)*mask_p))\n",
    "        if idxs_to_mask.size > 0: self.replace_with_mask(idxs_to_mask)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_tokens(ids): \n",
    "    ids = listify(to_np(ids))\n",
    "    return [tokenizer.id_to_token(o) for o in ids]\n",
    "tokenizer.convert_ids_to_tokens = convert_ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(range(len(train_inputs)))\n",
    "input_ids = tensor(train_inputs[i]['input_ids'])\n",
    "attention_mask = tensor(train_inputs[i]['attention_mask'])\n",
    "start_position, end_position = train_inputs[i]['start_end_tok_idxs']\n",
    "start_position, end_position = tensor(start_position), tensor(end_position)\n",
    "answer_text = train_inputs[i]['answer_text']\n",
    "context_text = train_inputs[i]['context_text']\n",
    "offsets = train_inputs[i]['offsets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  2430,     2,     2,    24, 23330,  1290,   162,     6,    47,\n",
       "        12905,   241, 44736,    34,  1714,     4,    24, 12905,    29,    95,\n",
       "          101,   358,    97, 44736,  4607,     6,    47,   341,     7,    28,\n",
       "          430,     2])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[attention_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(6))"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position, end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saddens',\n",
       " 'It saddens me, you`re youtube has changed. It`s just like every other youtube kid, you used to be different',\n",
       " 5,\n",
       " 6)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_text, context_text, start_position.item(), end_position.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Ġnegative </s> </s> Ġit Ġsadd ens Ġme , Ġyou ` re Ġyoutube Ġhas Ġchanged . Ġit ` s Ġjust Ġlike Ġevery Ġother Ġyoutube Ġkid , Ġyou Ġused Ġto Ġbe Ġdifferent </s>'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([tokenizer.id_to_token(o) for o in input_ids[attention_mask.bool()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġsadd ens'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([tokenizer.id_to_token(o) for o in input_ids[start_position.item(): end_position.item()+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_start = min(np.concatenate([offsets[start_position.item()], offsets[end_position.item()]]))\n",
    "char_end = max(np.concatenate([offsets[start_position.item()], offsets[end_position.item()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saddens'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text[char_start:char_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo right truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 1313,    2,    2,   16,  298, 8578,  127,  657, 1942,    2])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.input_ids[da.attention_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Ġnegative </s> </s> Ġit Ġsadd ens Ġme , Ġyou ` re Ġyoutube Ġhas Ġchanged . Ġit ` s Ġjust Ġlike Ġevery Ġother Ġyoutube </s>\n",
      "\n",
      "Ġsadd ens\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position)\n",
    "da.random_right_truncate()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo left truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Ġnegative </s> </s> Ġit Ġsadd ens Ġme , Ġyou ` re Ġyoutube Ġhas Ġchanged . Ġit ` s Ġjust Ġlike Ġevery Ġother Ġyoutube Ġkid , Ġyou Ġused Ġto Ġbe Ġdifferent </s>\n",
      "\n",
      "Ġsadd ens\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position)\n",
    "da.random_left_truncate()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.ans_start_pos, da.ans_end_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo replace with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Ġnegative </s> </s> Ġit Ġsadd ens Ġme , Ġyou ` <mask> Ġyoutube Ġhas Ġchanged . Ġit ` s Ġjust Ġlike Ġevery Ġother <mask> Ġkid , Ġyou Ġused <mask> <mask> Ġdifferent </s>\n",
      "\n",
      "Ġsadd ens\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position)\n",
    "da.random_replace_with_mask(0.2)\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4]),\n",
       " array([ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]))"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.left_idxs, da.right_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Ġnegative </s> </s> Ġit Ġsadd ens Ġme , <mask> ` re </s>\n",
      "\n",
      "Ġsadd ens\n"
     ]
    }
   ],
   "source": [
    "da = TSEDataAugmentor(tokenizer, input_ids, attention_mask, start_position, end_position)\n",
    "\n",
    "da.random_left_truncate()\n",
    "da.random_right_truncate()\n",
    "da.random_replace_with_mask(0.3)\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.attention_mask.bool()])))\n",
    "print()\n",
    "print(\" \".join(tokenizer.convert_ids_to_tokens(da.input_ids[da.ans_start_pos :da.ans_end_pos+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSEDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_left_truncate': {'p': 0.3},\n",
       " 'random_right_truncate': {'p': 0.3},\n",
       " 'random_replace_with_mask': {'p': 0.3, 'mask_p': 0.2}}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "do_tfms = {}\n",
    "do_tfms[\"random_left_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_right_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_replace_with_mask\"] = {\"p\":.3, \"mask_p\":0.2}\n",
    "do_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSEDataset(Dataset):\n",
    "    def __init__(self, inputs, tokenizer=None, is_training=True, do_tfms:Dict=None):\n",
    "\n",
    "        # eval\n",
    "        self.inputs = inputs\n",
    "#         answer_text = train_inputs[i]['answer_text']\n",
    "#         context_text = train_inputs[i]['context_text']\n",
    "#         offsets = train_inputs[i]['offsets']\n",
    "        # augmentation\n",
    "        self.is_training = is_training\n",
    "        self.tokenizer = tokenizer\n",
    "        self.do_tfms = do_tfms\n",
    "                \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        'fastai requires (xb, yb) to return'\n",
    "        \n",
    "        input_ids = tensor(self.inputs[i]['input_ids'])\n",
    "        attention_mask = tensor(self.inputs[i]['attention_mask'])\n",
    "    \n",
    "        if self.is_training: \n",
    "            start_position, end_position = self.inputs[i]['start_end_tok_idxs']\n",
    "            start_position, end_position = tensor(start_position), tensor(end_position)\n",
    "            \n",
    "            if self.do_tfms:\n",
    "                augmentor = TSEDataAugmentor(self.tokenizer, input_ids,\n",
    "                                             attention_mask, start_position, end_position)\n",
    "\n",
    "                if np.random.uniform() < self.do_tfms[\"random_left_truncate\"][\"p\"]:\n",
    "                    augmentor.random_left_truncate()\n",
    "                if np.random.uniform() < self.do_tfms[\"random_right_truncate\"][\"p\"]:\n",
    "                    augmentor.random_right_truncate()\n",
    "                if np.random.uniform() < self.do_tfms[\"random_replace_with_mask\"][\"p\"]:\n",
    "                    augmentor.random_replace_with_mask(self.do_tfms[\"random_replace_with_mask\"][\"mask_p\"])\n",
    "\n",
    "                input_ids = augmentor.input_ids\n",
    "                attention_mask = augmentor.attention_mask\n",
    "                start_position, end_position = tensor(augmentor.ans_start_pos), tensor(augmentor.ans_end_pos)\n",
    "                \n",
    "            \n",
    "        xb = (input_ids, attention_mask)\n",
    "        if self.is_training: yb = (start_position, end_position)\n",
    "        else: yb = (0,0)\n",
    "        \n",
    "        return xb, yb\n",
    "    \n",
    "    def __len__(self): return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TSEDataset(train_inputs, tokenizer, True, do_tfms=do_tfms)\n",
    "test_ds = TSEDataset(test_inputs, tokenizer, False, do_tfms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([    0,  2430,     2,     2,    24, 23330,  1290,   162,     6,    47,\n",
       "          12905, 50264, 44736, 50264,  1714,     4,    24, 12905,    29, 50264,\n",
       "            101, 50264,    97, 44736,  4607,     6,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       " (tensor(5), tensor(6)))"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict_answer_text`\n",
    "\n",
    "TODO: Migrate to proper notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict_answer_text(start_logits, end_logits, attention_mask,\n",
    "                        context_text, char_to_word_offset, token_to_orig_map): \n",
    "    \"Find best answer from context\"\n",
    "    # find best start and end\n",
    "    context_start, context_end = min(token_to_orig_map), max(token_to_orig_map)\n",
    "    truncated_start_logits = start_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "    truncated_end_logits = end_logits[attention_mask.bool()][context_start:context_end+1]\n",
    "    best_start_idx, best_end_idx = find_best_start_end_idxs(truncated_start_logits, truncated_end_logits)\n",
    "    \n",
    "    # generate answer\n",
    "    tok_orig_char_start = token_to_orig_map[best_start_idx+context_start] \n",
    "    tok_orig_char_end = token_to_orig_map[best_end_idx+context_start]\n",
    "    return answer_from_orig_context(context_text, char_to_word_offset, tok_orig_char_start, tok_orig_char_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edings for the baby are fun when he is a'"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_text(start_logits, end_logits, attention_mask, \n",
    "                   context_text, char_to_word_offset, token_to_orig_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n",
      "Converted 04-models.ipynb.\n",
      "Converted post-process.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse] *",
   "language": "python",
   "name": "conda-env-tse-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
