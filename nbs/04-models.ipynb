{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers: 0.7.0\n",
      "fastai: 1.0.60\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from fastai.text import *\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from tse.preprocessing import *\n",
    "from tse.tokenizers import *\n",
    "from tse.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_roberta_model(path_to_dir=\"../roberta-base/\"):\n",
    "    conf = RobertaConfig.from_pretrained(path_to_dir)\n",
    "    conf.output_hidden_states = True\n",
    "    model = RobertaModel.from_pretrained(path_to_dir, config=conf)\n",
    "    # outputs: (final_hidden, pooled_final_hidden, (embedding + 12 hidden))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_roberta_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_text(train_df, \"text\")\n",
    "strip_text(train_df, \"selected_text\")\n",
    "strip_text(test_df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_whitespace(train_df, \"text\")\n",
    "replace_whitespace(train_df, \"selected_text\")\n",
    "replace_whitespace(test_df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_URLs(train_df, \"text\")\n",
    "replace_URLs(train_df, \"selected_text\")\n",
    "replace_URLs(test_df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_user(train_df, \"text\")\n",
    "replace_user(train_df, \"selected_text\")\n",
    "replace_user(test_df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_wrong = train_df.apply(lambda o: is_wrong_selection(o['text'], o['selected_text']), 1)\n",
    "train_df = train_df[~is_wrong].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "fold_idxs = list(kfold.split(train_df))\n",
    "for i, (trn_idx, val_idx) in enumerate(fold_idxs): train_df.loc[val_idx, \"val_fold\"] = int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_roberta_tokenizer(\"../roberta-base/vocab.json\", \"../roberta-base/merges.txt\", 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    5492\n",
       "0.0    5492\n",
       "3.0    5492\n",
       "1.0    5492\n",
       "4.0    5491\n",
       "Name: val_fold, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.val_fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold dfs\n",
    "trn_df = train_df[train_df['val_fold'] != 0]\n",
    "val_df = train_df[train_df['val_fold'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold inputs\n",
    "train_inputs = QAInputGenerator.from_df(trn_df, tokenizer=tokenizer)\n",
    "valid_inputs = QAInputGenerator.from_df(val_df, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_left_truncate': {'p': 0.3},\n",
       " 'random_right_truncate': {'p': 0.3},\n",
       " 'random_replace_with_mask': {'p': 0.3, 'mask_p': 0.2}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "do_tfms = {}\n",
    "do_tfms[\"random_left_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_right_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_replace_with_mask\"] = {\"p\":.3, \"mask_p\":0.2}\n",
    "do_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold ds\n",
    "train_ds = TSEDataset(train_inputs, tokenizer, is_test=False, do_tfms=do_tfms)\n",
    "valid_ds = TSEDataset(train_inputs, tokenizer, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([    0,  2430,     2,     2,    98,  3036,  5074,   939,    40,  2649,\n",
       "             47,   259,    11, 15610,  1597,  2977, 16506,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       " (tensor(4), tensor(6)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=32, val_bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[   0, 7974,    2,  ...,    1,    1,    1],\n",
       "          [   0, 2430,    2,  ...,    1,    1,    1],\n",
       "          [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "          ...,\n",
       "          [   0, 7974,    2,  ...,    1,    1,    1],\n",
       "          [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "          [   0, 1313,    2,  ...,    1,    1,    1]]),\n",
       "  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])],\n",
       " [tensor([ 4, 13, 28,  4, 33,  4,  5,  4, 14,  4,  8,  8,  6,  4, 12,  7,  4,  7,\n",
       "          14, 17,  9,  6,  5,  4,  4, 12, 11, 10, 13,  4,  4, 32]),\n",
       "  tensor([22, 26, 28, 39, 34,  7,  5,  4, 18, 40,  8,  8,  7, 30, 12, 18, 10,  7,\n",
       "          34, 26, 12,  7, 11,  7,  8, 13, 14, 12, 14, 20,  5, 32])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "noop_layer = Lambda(lambda x: x)\n",
    "\n",
    "class QAHead(Module): \n",
    "    def __init__(self, p=0.5, hidden_size=768, num_hidden_states=2, use_ln=False):  \n",
    "        self.ln0 = nn.LayerNorm(hidden_size*num_hidden_states) if use_ln else noop_layer\n",
    "        self.d0 = nn.Dropout(p)\n",
    "        self.l0 = nn.Linear(hidden_size*num_hidden_states, 2)\n",
    "    def forward(self, x):\n",
    "        return self.l0(self.d0(self.ln0(x)))\n",
    "    \n",
    "class TSEModel(Module):\n",
    "    def __init__(self, pretrained_model, head=QAHead(), num_hidden_states=2): \n",
    "        self.sequence_model = pretrained_model\n",
    "        self.head = head\n",
    "        self.num_hidden_states = num_hidden_states\n",
    "        \n",
    "    def forward(self, *xargs):\n",
    "        inp = {}\n",
    "        inp[\"input_ids\"] = xargs[0]\n",
    "        inp[\"attention_mask\"] = xargs[1]    \n",
    "        _, _, hidden_states = self.sequence_model(**inp)\n",
    "        x = torch.cat(hidden_states[-self.num_hidden_states:], dim=-1)\n",
    "        start_logits, end_logits = self.head(x).split(1, dim=-1)\n",
    "        return (start_logits.squeeze(-1), end_logits.squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tse_model = TSEModel(model, QAHead(use_ln=True, num_hidden_states=2), num_hidden_states=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = tse_model(*xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8597,  0.5541,  0.8427,  ..., -0.2491, -0.1137, -0.1327],\n",
       "         [ 0.6116, -1.3524,  0.0248,  ...,  0.6115,  0.8474, -0.9773],\n",
       "         [ 0.7890,  0.1800,  0.9404,  ...,  0.1278,  0.0720,  0.8742],\n",
       "         ...,\n",
       "         [ 0.6648, -0.7379,  0.6890,  ..., -0.5549,  0.1579,  0.1812],\n",
       "         [ 0.7905,  0.2779,  0.0317,  ...,  1.1267,  0.9727, -0.0054],\n",
       "         [-0.0068, -0.1116,  0.7793,  ...,  0.2527, -0.1828, -0.8484]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ 0.6134,  1.2737, -0.0136,  ..., -0.6596, -0.1256, -0.0377],\n",
       "         [ 0.6160,  0.4515, -0.0087,  ...,  0.4548,  0.3897,  0.3446],\n",
       "         [ 0.6111,  0.6305,  0.1408,  ...,  1.0373,  0.4215,  0.3342],\n",
       "         ...,\n",
       "         [ 0.5712,  0.1099,  0.6075,  ...,  0.7023,  0.2611,  0.2541],\n",
       "         [ 0.6227,  0.8035,  0.4469,  ...,  0.9531,  1.4756,  0.9023],\n",
       "         [-0.1501,  1.2173,  0.6362,  ...,  0.0563,  0.8055,  1.0089]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CELoss(Module):\n",
    "    \"single backward by concatenating both start and logits with correct targets\"\n",
    "    def __init__(self): self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs\n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3878, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CELoss()\n",
    "loss_fn(out, *yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSLoss(Module):\n",
    "    \"single backward by concatenating both start and logits with correct targets\"\n",
    "    def __init__(self, eps=0.1): self.loss_fn = LabelSmoothingCrossEntropy(eps=eps)\n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs\n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3878, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = LSLoss()\n",
    "loss_fn(out, *yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_best_start_end_idxs(_start_logits, _end_logits):\n",
    "    best_logit = -1000\n",
    "    best_idxs = None\n",
    "    for start_idx, start_logit in enumerate(_start_logits):\n",
    "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
    "            logit_sum = (start_logit + end_logit).item()\n",
    "            if logit_sum > best_logit:\n",
    "                best_logit = logit_sum\n",
    "                best_idxs = (start_idx, start_idx+end_idx)\n",
    "    return best_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([   0, 7974,    2,    2, ...,    1,    1,    1,    1]),\n",
       " 'attention_mask': array([1, 1, 1, 1, ..., 0, 0, 0, 0]),\n",
       " 'offsets': array([[0, 0],\n",
       "        [0, 7],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]),\n",
       " 'tokens': array(['<s>', 'Ġneutral', '</s>', '</s>', ..., '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U10'),\n",
       " 'context_text': 'I`d have responded, if I were going',\n",
       " 'answer_text': 'I`d have responded, if I were going',\n",
       " 'start_end_tok_idxs': (4, 13),\n",
       " 'text_id': 'cb774db0d1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 7974,    2,  ...,    1,    1,    1],\n",
       "        [   0, 2430,    2,  ...,    1,    1,    1],\n",
       "        [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   0, 7974,    2,  ...,    1,    1,    1],\n",
       "        [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "        [   0, 1313,    2,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([   0, 7974,    2,    2, ...,    1,    1,    1,    1]),\n",
       " 'attention_mask': array([1, 1, 1, 1, ..., 0, 0, 0, 0]),\n",
       " 'offsets': array([[0, 0],\n",
       "        [0, 7],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]),\n",
       " 'tokens': array(['<s>', 'Ġneutral', '</s>', '</s>', ..., '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U10'),\n",
       " 'context_text': 'I`d have responded, if I were going',\n",
       " 'answer_text': 'I`d have responded, if I were going',\n",
       " 'start_end_tok_idxs': (4, 13),\n",
       " 'text_id': 'cb774db0d1'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class JaccardScore(Callback):\n",
    "    \"Stores predictions and targets to perform calculations on epoch end.\"\n",
    "    def __init__(self, valid_ds): \n",
    "        self.valid_ds = valid_ds\n",
    "        self.offset_shift = 4\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.jaccard_scores = []  \n",
    "        self.valid_ds_idx = 0\n",
    "        \n",
    "        \n",
    "    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "                \n",
    "        input_ids, attention_masks = last_input[0], last_input[1].bool()\n",
    "        start_logits, end_logits = last_output\n",
    "        \n",
    "        \n",
    "        # mask select only context part\n",
    "        for i in range(len(input_ids)):\n",
    "\n",
    "            _input_ids = input_ids[i].masked_select(attention_masks[i])\n",
    "            _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] \n",
    "            _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] \n",
    "            start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n",
    "            start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n",
    "            \n",
    "            context_text = self.valid_ds.inputs[self.valid_ds_idx]['context_text']\n",
    "            offsets = self.valid_ds.inputs[self.valid_ds_idx]['offsets']\n",
    "            answer_text = self.valid_ds.inputs[self.valid_ds_idx]['answer_text']\n",
    "            \n",
    "            start_offs, end_offs = offsets[start_idx], offsets[end_idx]\n",
    "            answer = context_text[start_offs[0]:end_offs[1]]            \n",
    "            \n",
    "            self.jaccard_scores.append(jaccard(answer, answer_text))\n",
    "            self.valid_ds_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):        \n",
    "        res = np.mean(self.jaccard_scores)\n",
    "        return add_metrics(last_metrics, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_split_func(m, num_hidden_states): \n",
    "    \"4 layer groups\"\n",
    "    return (m.sequence_model.embeddings, \n",
    "            m.sequence_model.encoder.layer[:-num_hidden_states],\n",
    "            m.sequence_model.encoder.layer[-num_hidden_states:],\n",
    "            m.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(data, tse_model, loss_func=CELoss(), metrics=[JaccardScore(valid_ds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSEModel(\n",
       "  (sequence_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (head): QAHead(\n",
       "    (ln0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (d0): Dropout(p=0.5, inplace=False)\n",
       "    (l0): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fn = partial(model_split_func, num_hidden_states=2)\n",
    "learner = learner.split(split_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop_cb = EarlyStoppingCallback(learner, monitor='jaccard_score',mode='max',patience=2)\n",
    "# save_model_cb = SaveModelCallback(learner,every='improvement',monitor='jaccard_score',name=f'{MODEL_TYPE}-qa-finetune')\n",
    "# csv_logger_cb = CSVLogger(learner, f\"training_logs_{foldnum}\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.to_fp16();\n",
    "# learner.to_fp32();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-preprocessing.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n",
      "Converted 04-models.ipynb.\n",
      "Converted post-process.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse]",
   "language": "python",
   "name": "conda-env-tse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
