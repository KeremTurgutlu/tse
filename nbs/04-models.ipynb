{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers: 0.7.0\n",
      "fastai: 1.0.60\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from fastai.text import *\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from tse.tokenizers import *\n",
    "from tse.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_roberta_model(path_to_dir=\"../roberta-base/\"):\n",
    "    conf = RobertaConfig.from_pretrained(path_to_dir)\n",
    "    conf.output_hidden_states = True\n",
    "    model = RobertaModel.from_pretrained(path_to_dir, config=conf)\n",
    "    # outputs: (final_hidden, pooled_final_hidden, (embedding + 12 hidden))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_roberta_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and strip data\n",
    "train_df = pd.read_csv(\"../data/train.csv\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "train_df.selected_text = train_df.selected_text.apply(lambda s: s.strip())\n",
    "train_df.text = train_df.text.apply(lambda s: s.strip())\n",
    "test_df.text = test_df.text.apply(lambda s: s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "fold_idxs = list(kfold.split(train_df))\n",
    "for i, (trn_idx, val_idx) in enumerate(fold_idxs): train_df.loc[val_idx, \"val_fold\"] = int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_roberta_tokenizer(\"../roberta-base/vocab.json\", \"../roberta-base/merges.txt\", 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold dfs\n",
    "trn_df = train_df[train_df['val_fold'] != 0]\n",
    "val_df = train_df[train_df['val_fold'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold inputs\n",
    "train_inputs = QAInputGenerator.from_df(trn_df, tokenizer=tokenizer)\n",
    "valid_inputs = QAInputGenerator.from_df(val_df, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_left_truncate': {'p': 0.3},\n",
       " 'random_right_truncate': {'p': 0.3},\n",
       " 'random_replace_with_mask': {'p': 0.3, 'mask_p': 0.2}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "do_tfms = {}\n",
    "do_tfms[\"random_left_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_right_truncate\"] = {\"p\":.3}\n",
    "do_tfms[\"random_replace_with_mask\"] = {\"p\":.3, \"mask_p\":0.2}\n",
    "do_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold ds\n",
    "train_ds = TSEDataset(train_inputs, tokenizer, True, do_tfms=do_tfms)\n",
    "valid_ds = TSEDataset(train_inputs, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([   0, 2430,    2,    2,   98, 3036, 5074,  939,   40, 2649,   47,  259,\n",
       "             2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       " (tensor(4), tensor(6)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=32, val_bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[   0, 1313,    2,  ...,    1,    1,    1],\n",
       "          [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "          [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "          ...,\n",
       "          [   0, 2430,    2,  ...,    1,    1,    1],\n",
       "          [   0, 7974,    2,  ...,    1,    1,    1],\n",
       "          [   0, 2430,    2,  ...,    1,    1,    1]]),\n",
       "  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])],\n",
       " [tensor([ 4,  4,  9,  4,  4,  4, 10,  5, 19, 31,  7,  4,  4, 12,  6,  4,  4,  4,\n",
       "           4, 11,  4,  4,  4,  6,  5,  4,  4,  4,  6,  8,  4,  5]),\n",
       "  tensor([13, 26,  9,  5, 47, 10, 10, 30, 23, 31,  9,  4,  6, 12,  7, 35,  9, 25,\n",
       "          24, 30,  9,  7, 10,  6,  5, 41, 29, 27,  6, 10, 10,  5])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QAHead(Module): \n",
    "    def __init__(self, p=0.5):    \n",
    "        self.d0 = nn.Dropout(p)\n",
    "        self.l0 = nn.Linear(768*2, 2)\n",
    "    def forward(self, x):\n",
    "        return self.l0(self.d0(x))\n",
    "    \n",
    "class TSEModel(Module):\n",
    "    def __init__(self, model): \n",
    "        self.sequence_model = model\n",
    "        self.head = QAHead()\n",
    "        \n",
    "    def forward(self, *xargs):\n",
    "        inp = {}\n",
    "        inp[\"input_ids\"] = xargs[0]\n",
    "        inp[\"attention_mask\"] = xargs[1]    \n",
    "        _, _, hidden_states = self.sequence_model(**inp)\n",
    "        x = torch.cat([hidden_states[-1], hidden_states[-1]], dim=-1)\n",
    "        start_logits, end_logits = self.head(x).split(1, dim=-1)\n",
    "        return (start_logits.squeeze(-1), end_logits.squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tse_model = TSEModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tse_model(*xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CELoss(Module):\n",
    "    \"single backward by concatenating both start and logits with correct targets\"\n",
    "    def __init__(self): self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs\n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2267, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CELoss()\n",
    "loss_fn(out, *yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSLoss(Module):\n",
    "    \"single backward by concatenating both start and logits with correct targets\"\n",
    "    def __init__(self, eps=0.1): self.loss_fn = LabelSmoothingCrossEntropy(eps=eps)\n",
    "    def forward(self, inputs, start_targets, end_targets):\n",
    "        start_logits, end_logits = inputs\n",
    "        logits = torch.cat([start_logits, end_logits]).contiguous()\n",
    "        targets = torch.cat([start_targets, end_targets]).contiguous()\n",
    "        return self.loss_fn(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2330, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = LSLoss()\n",
    "loss_fn(out, *yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_best_start_end_idxs(_start_logits, _end_logits):\n",
    "    best_logit = -1000\n",
    "    best_idxs = None\n",
    "    for start_idx, start_logit in enumerate(_start_logits):\n",
    "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
    "            logit_sum = (start_logit + end_logit).item()\n",
    "            if logit_sum > best_logit:\n",
    "                best_logit = logit_sum\n",
    "                best_idxs = (start_idx, start_idx+end_idx)\n",
    "    return best_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([   0, 7974,    2,    2, ...,    1,    1,    1,    1]),\n",
       " 'attention_mask': array([1, 1, 1, 1, ..., 0, 0, 0, 0]),\n",
       " 'offsets': array([[0, 0],\n",
       "        [0, 7],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]),\n",
       " 'tokens': array(['<s>', 'Ġneutral', '</s>', '</s>', ..., '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U10'),\n",
       " 'context_text': 'I`d have responded, if I were going',\n",
       " 'answer_text': 'I`d have responded, if I were going',\n",
       " 'start_end_tok_idxs': (4, 13)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 1313,    2,  ...,    1,    1,    1],\n",
       "        [   0, 7974,    2,  ...,    1,    1,    1],\n",
       "        [   0, 1313,    2,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   0, 7974,    2,  ...,    1,    1,    1],\n",
       "        [   0, 2430,    2,  ...,    1,    1,    1],\n",
       "        [   0, 7974,    2,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([   0, 7974,    2,    2, ...,    1,    1,    1,    1]),\n",
       " 'attention_mask': array([1, 1, 1, 1, ..., 0, 0, 0, 0]),\n",
       " 'offsets': array([[0, 0],\n",
       "        [0, 7],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]),\n",
       " 'tokens': array(['<s>', 'Ġneutral', '</s>', '</s>', ..., '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U10'),\n",
       " 'context_text': 'I`d have responded, if I were going',\n",
       " 'answer_text': 'I`d have responded, if I were going',\n",
       " 'start_end_tok_idxs': (4, 13)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class JaccardScore(Callback):\n",
    "    \"Stores predictions and targets to perform calculations on epoch end.\"\n",
    "    def __init__(self, valid_ds): \n",
    "        self.valid_ds = valid_ds\n",
    "        self.offset_shift = 4\n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.jaccard_scores = []  \n",
    "        self.valid_ds_idx = 0\n",
    "        \n",
    "        \n",
    "    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "                \n",
    "        input_ids, attention_masks = last_input[0], last_input[1].bool()\n",
    "        start_logits, end_logits = last_output\n",
    "        \n",
    "        \n",
    "        # mask select only context part\n",
    "        for i in range(len(input_ids)):\n",
    "\n",
    "            _input_ids = input_ids[i].masked_select(attention_masks[i])\n",
    "            _start_logits = start_logits[i].masked_select(attention_masks[i])[4:-1] \n",
    "            _end_logits = end_logits[i].masked_select(attention_masks[i])[4:-1] \n",
    "            start_idx, end_idx = get_best_start_end_idxs(_start_logits, _end_logits)\n",
    "            start_idx, end_idx = start_idx + self.offset_shift, end_idx + self.offset_shift\n",
    "            \n",
    "            context_text = self.valid_ds.inputs[self.valid_ds_idx]['context_text']\n",
    "            offsets = self.valid_ds.inputs[self.valid_ds_idx]['offsets']\n",
    "            answer_text = self.valid_ds.inputs[self.valid_ds_idx]['answer_text']\n",
    "            \n",
    "            start_offs, end_offs = offsets[start_idx], offsets[end_idx]\n",
    "            answer = context_text[start_offs[0]:end_offs[1]]            \n",
    "            \n",
    "            self.jaccard_scores.append(jaccard(answer, answer_text))\n",
    "            self.valid_ds_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):        \n",
    "        res = np.mean(self.jaccard_scores)\n",
    "        return add_metrics(last_metrics, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def model_split_func(m): \n",
    "    \"4 layer groups\"\n",
    "    n = (2*len(m.sequence_model.encoder.layer))//3 \n",
    "    return (m.sequence_model.embeddings, \n",
    "            m.sequence_model.encoder.layer[:n],\n",
    "            m.sequence_model.encoder.layer[n:],\n",
    "            m.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(data, tse_model, loss_func=CELoss(), metrics=[JaccardScore(valid_ds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.split(model_split_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop_cb = EarlyStoppingCallback(learner, monitor='jaccard_score',mode='max',patience=2)\n",
    "# save_model_cb = SaveModelCallback(learner,every='improvement',monitor='jaccard_score',name=f'{MODEL_TYPE}-qa-finetune')\n",
    "# csv_logger_cb = CSVLogger(learner, f\"training_logs_{foldnum}\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.to_fp16();\n",
    "# learner.to_fp32();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01-squad-utils.ipynb.\n",
      "Converted 02-tokenizers.ipynb.\n",
      "Converted 03-datasets.ipynb.\n",
      "Converted 04-models.ipynb.\n",
      "Converted post-process.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tse]",
   "language": "python",
   "name": "conda-env-tse-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
